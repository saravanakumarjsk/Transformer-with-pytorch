{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "95pNInFu7eYW",
    "outputId": "42a4c97f-cb35-4602-b7e0-c5a91b430924"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src_vocab[ich]\n",
    "# for n in sentences[0].split():\n",
    "#   print(src_vocab[n])\n",
    "\n",
    "input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "D1cUwFo6wC21",
    "outputId": "80c779c8-cdcc-4643-dc62-26d3bd250c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 4, 0]]),\n",
       " tensor([[5, 1, 2, 3, 4]]),\n",
       " tensor([[1, 2, 3, 4, 6]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "make_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cAbJyglowYvL",
    "outputId": "6818fba2-9c71-48e5-8014-bb0a45aa4d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input   tensor([[1, 2, 3, 4, 0]])\n"
     ]
    }
   ],
   "source": [
    "enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "print('Encoder input  ', enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attn_key_pad_mask(seq_k,     seq_q):\n",
    "# to mask the padding size of the sequence k\n",
    "    len_q = seq_q.size(1) #\n",
    "    padding_mask = seq_k.eq(0)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)\n",
    "    return padding_mask\n",
    "\n",
    "get_attn_key_pad_mask(enc_inputs, enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "tensor([[[0, 0, 0, 0, 1]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "padding_mask = enc_inputs.eq(0)\n",
    "print(padding_mask)\n",
    "padding_mask = padding_mask.unsqueeze(1)#.expand(-1, 5, -1)\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "U0AX0DCu1Rng",
    "outputId": "59561c2e-8fcc-4269-a7d2-e82b1a901f6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = enc_inputs.data.eq(0)\n",
    "a\n",
    "a.expand(1, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaSbnhy2xVMp"
   },
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0]])\n",
      "tensor([[0, 0, 0, 0, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(enc_inputs)\n",
    "le = enc_inputs.size(1)\n",
    "padding_mask = enc_inputs.eq(0)\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pad_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(0).type(torch.float).unsqueeze(-1)\n",
    "get_pad_mask(enc_inputs)\n",
    "#print(enc_inputs.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "tensor([[0, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "shape = [5,5]\n",
    "sub_mask = np.triu(np.ones(shape), k=1)\n",
    "print(sub_mask)\n",
    "mask = torch.from_numpy(sub_mask).byte()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "tensor([[5, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(bat)\n",
    "print(lq)\n",
    "print(dec_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lFDvR1EQ4YTh",
    "outputId": "2e11b45e-3ff9-427c-9529-a607950d4e54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "vBrYIQWVw_B_",
    "outputId": "53ae49d5-32cf-4bfe-edec-ee7b85547ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1]]], dtype=torch.uint8)\n",
      "\n",
      " tensor([[[0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "new = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "print(new)\n",
    "hey = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "print('\\n', hey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "wpUU1orvxcx6",
    "outputId": "c08689f6-fa4f-44a3-f0c7-5341f9734ce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "  \n",
    "get_attn_subsequent_mask(dec_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "gc1Npcqh7BVH",
    "outputId": "0dc44eeb-a1e4-4cdf-8b3d-0bde657e1f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.triu(np.ones((1,5,5)), k=1)\n",
    "print(a)\n",
    "b = torch.from_numpy(a).byte()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "2iqlICqKxepe",
    "outputId": "ccc00d09-be18-4995-da01-2d7d935513a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few = get_attn_subsequent_mask(dec_inputs)\n",
    "few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t8PG9vzYBvvC",
    "outputId": "9049505d-80f4-4e57-c9e5-a6b22b4e6389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.0,\n",
       " 5.0,\n",
       " 0.7924465962305566,\n",
       " 0.7924465962305566,\n",
       " 0.12559432157547898,\n",
       " 0.12559432157547898,\n",
       " 0.019905358527674867,\n",
       " 0.019905358527674867,\n",
       " 0.003154786722400965,\n",
       " 0.003154786722400965]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posistion = 5\n",
    "n_posistion = 5\n",
    "d_model = 10\n",
    "\n",
    "def cal_angle(position, hid_idx):\n",
    "    return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "\n",
    "\n",
    "def get_posi_angle_vec(position):\n",
    "    return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "\n",
    "print(cal_angle(5, 60))\n",
    "get_posi_angle_vec(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRQo7DYh_lzV"
   },
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "      \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "      \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "  \n",
    "a = get_sinusoid_encoding_table(5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "gzojyoqMBLrh",
    "outputId": "5e711e57-7936-4616-e584-52be929e5e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  5.40302306e-01  9.64661620e-01 ...  9.99999994e-01\n",
      "   1.03663293e-04  9.99999995e-01]\n",
      " [ 2.00000000e+00 -4.16146837e-01  1.92932324e+00 ...  9.99999977e-01\n",
      "   2.07326586e-04  9.99999979e-01]\n",
      " ...\n",
      " [ 7.70000000e+01 -3.09750317e-02  7.42789447e+01 ...  9.99965767e-01\n",
      "   7.98207355e-03  9.99968143e-01]\n",
      " [ 7.80000000e+01 -8.57803093e-01  7.52436064e+01 ...  9.99964872e-01\n",
      "   8.08573684e-03  9.99967311e-01]\n",
      " [ 7.90000000e+01 -8.95970947e-01  7.62082680e+01 ...  9.99963965e-01\n",
      "   8.18940013e-03  9.99966467e-01]]\n"
     ]
    }
   ],
   "source": [
    "sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(80)])\n",
    "sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "print(sinusoid_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9cd9a58182ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 5 were given"
     ]
    }
   ],
   "source": [
    "a= MultiHeadAttention(enc_inputs, enc_inputs, enc_inputs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1956
    },
    "colab_type": "code",
    "id": "TF9y4SW02G56",
    "outputId": "93974b57-0df8-4fc9-87aa-cbf960189d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.019113\n",
      "Epoch: 0002 cost = 0.269599\n",
      "Epoch: 0003 cost = 0.732250\n",
      "Epoch: 0004 cost = 0.123361\n",
      "Epoch: 0005 cost = 0.240645\n",
      "Epoch: 0006 cost = 0.008461\n",
      "Epoch: 0007 cost = 0.021096\n",
      "Epoch: 0008 cost = 0.026800\n",
      "Epoch: 0009 cost = 0.001937\n",
      "Epoch: 0010 cost = 0.004960\n",
      "Epoch: 0011 cost = 0.005663\n",
      "Epoch: 0012 cost = 0.003651\n",
      "Epoch: 0013 cost = 0.019557\n",
      "Epoch: 0014 cost = 0.004512\n",
      "Epoch: 0015 cost = 0.012752\n",
      "Epoch: 0016 cost = 0.001018\n",
      "Epoch: 0017 cost = 0.000684\n",
      "Epoch: 0018 cost = 0.000541\n",
      "Epoch: 0019 cost = 0.000567\n",
      "Epoch: 0020 cost = 0.002010\n",
      "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n",
      "first head of last state enc_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAIACAYAAABEsY85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFy1JREFUeJzt3Xu0rHdd3/HPN+cEQgR0mYSGhEAkggXKpXgAEbmVLAO4lotFEatAixQCiJWLFrUKopbF4iaXBsRTgdAaVIq25WJFaJKKCIQAS0pTBbmGAObkUiAEQxJ+/WPmkO24k3POPpl55jt5vdaadfZ+ZvbJdz9r57z375lnnqkxRgCAHo6YegAA4OAJNwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLctFZVP11V/6eqrqyqO863/WJVPXbq2QCWQbhpq6qeleRXkuxNUlvuuijJz0wyFMCSCTedPS3JU8YYr0pyzZbtH0lyt2lGAlgu4aazOyT5+Dbbr05yixXPArASwk1nn05y7222PzLJBSueBWAldk89AByGlyU5o6qOzuw57vtX1ROSPDfJkyadDGBJaowx9QywY1X1lMxOUDtpvumiJC8YY7x+uqkAlke42QhVdWySI8YYF089C8AyeY6btqrq7Kr6riQZY1yyP9pVdeuqOnva6QCWw4qbtqrqW0mOX1xlV9Vtklw0xjhymskAlsfJabRTVVvPJL9HVV225fNdSU7L7LlugI1jxU0785X2/h/c2uYh30jyb8YYb1jdVACrYcVNR9+TWbA/neS+SfZtue+bSS4eY1w7xWAAy2bFDQCNWHHTWlWdlOSBSW6ThVdJjDF+c5KhAJbIipu2qupxSd6Q2RuM7Mt1z3snyRhj3HGSwQCWSLhpq6o+leQPkjzPc9rATYVw01ZVXZHkHmOMT089C8CquHIanf1xkvtNPQTAKjk5jVaq6tFbPn13khdX1d2S/O/M3of728YYf7TK2QBWwaFyWplffOVgjDHGrqUOAzAB4QaARjzHDQCNCDdtVdUbqurnttn+nKr6nSlmAlg24aazRybZ7n23z57fB2ulqnZX1SOr6pipZ6Ev4aaz70pyxTbbv57ku1c8CxzQGOOaJH+U5FZTz0Jfwk1nn8j2K+sfSfI3K54FDtZfJvneqYegL6/jprOXJ3ldVd0m1x0yf1iSZyV5xmRTwQ17QZKXV9WvJvlwZkeIvm2McdkUQ9GHl4PRWlU9NcmvJDlxvumiJC8cY7xuuqng+i1ci2DrP8AV1x/gIAg3G6Gqjsvs5/niqWeBG1JVD76h+8cY/2tVs9CTcNNeVd0xyV0zW71cMMb4zMQjbYSqOjrJvbL9e527nCxMxHPctFVVt07y+iT/PMm3rttcf5jkX48xvjbZcM1V1alJfi/Jdi9bGkkczj0MVXX3JE9NckqSJ40xvlRVj0ryuTHGR6edjnXnrPIVqaqjq+oHq+pRVfXorbepZ2vsVUnukeShSW4xvz1svu2VE861CV6V5J1JbjfGOGLhJtqHoap+OMmHMjsv459l9nObzCL+q1PNRR8Ola/AgVYv/iHcmaq6NMmjxhjvXdj+oCT/dYzhIhc7VFVfz+y9zj819Sybpqo+mORNY4zXVtXXktxzjPHpqvr+JG8fY5ww8YisOSvu1bB6WY5bJLl0m+2XJTlqxbNsmvcl+b6ph9hQd8vsveQXXRYXDuIgeI57NU5O8qNjjC9OPciGeV+S36iqJ4wxrkySqvqOJL+W5C8mnay/1yV5WVWdkO3f6/wjk0y1GS7P7DD5Zxe23zvJF1Y+De0I92rsX7047HjjenaSP0lyUVV9LLOTpu6Z5MokPzzlYBvgrfM/925zn5PTDs+bk7y0qh6b2b7cPX+J2MuSvHHSyWjBc9xLUlX33vLpyUn+fZLfjNXLjaqqbpHkcUnuktkFLC5IctYY4xuTDtZcVd3hhu4fY3xuVbNsmqo6MsmZSf5FZj+z35r/+eYkTxxjXDvddHQg3EsyvzrSyOx/yBvi5LTDUFXHJ/nBbP9a49dOMhQchKo6Jck/zezn9qNjjE9OPBJNCPeSHGjFspXVy85U1eOT/E5mvxxdnr9/+cjh7NxDM39p4tvHGFcf6GWKLsAC0xFu2qqqzyV5U5Jfn79dIodhfpTo+DHGxQvX017kKNEhqqpXJ/mlMcbX5x9frzHGz65oLJpyctoKVNULk1y4+MYXVfW0JCeOMZ43zWTt3TrJmaJ94xhjHLHdx9wo7p7kyC0fXx8rKQ7IinsFqurzSX5sjPHBhe33SfLWMcZBH1bnOlV1RpK/HmP8h6ln2URV9YjM3h71jklOG2NcWFVPTvKZMcb/nHa6zVBVt0ySMcYVU89CH36rXo3bJNm3zfZLk/yjFc+ySZ6T5BFV9d+q6jeq6vlbb1MP11lVPS7JW5J8Msn35LrV4q4kz51qrk1RVc+a/0L/lSRfqaoLq+rZVXWgk1nZxvyS0q+pqouq6uKqenNVHTv1XMviUPlqfD7JA5N8emH7g+KCC4fjqUkenuSSJN+bhZPTkvz6FENtiOcmecoY4/fnq+z9PhD79bBU1UuSnJ7kpUneP998/yTPT3Lb+MVoJ34tyROTnJXkG0l+MslvJfmxCWdaGuFejd9O8oqqulmSs+fbHpbkRUlePNlU/T0vyc+NMV4x9SAb6E65LipbXZHZuQXs3JOTPHmM8dYt286uqr/O7N8K4T50j87sHQF/P0mq6qwk76uqXZv4unjhXoExxsvnh21eneRmmb186arMrmH+0ilna25XkrdNPcSG+mKSOydZfKnig+IKgDeGj13PNk9f7sxJSb79ZkNjjPOq6pokJyS5cLKplsQPyYqMMX4pybFJfmB+O26M8YvD2YGH442ZXTWNG9/eJK+uqgfMPz+pqv5VkpdkdgiSnftPmZ30t+jpSf7zimfZFLuSfHNh2zXZ0MXpRn5T66Cq3pbk8WOMr84/3u4xSZIxxo+ucrYNcnSSJ1fVaZmtVhYvJev1sDs0xnhJVX1nkndn9k5r52R2lOhlY4zXTDpcQwuv3d6d5PHzn9sPzLfdL7PV4Vmrnm1DVJLfraqrtmw7Ksl/rKor92/YlH9rhXt5Ls11J0tt99aTHL67JPno/ON/vHCfIxmHaYzxy/NrENw1s6NzF3jZ0o4tvnb7w/M/978U9Mvz2+LPMQfnTdts+92VT7EiXscNAI14jhsAGhHuFauq06eeYVPZt8tj3y6Pfbs8m7pvhXv1NvIHaU3Yt8tj3y6Pfbs8G7lvhRsAGtmIk9OO/e5d4+STjjzwA9fAvkuvzXHHeEfEZbBvl8e+XZ5u+/YTHzt66hEO2tW5Kkfm5lOPcdC+lssvGWMcd6DHbcTLwU4+6cic966Tph4DYOOddsK9ph5hY71nvHXxSoXbcqgcABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABpZ63BX1ZlV9Y6p5wCAdbF76gEO4JlJauohAGBdrHW4xxhfmXoGAFgnDpUDQCNrHW4A4O9rG+6qOr2qzq+q8/ddeu3U4wDASrQN9xhj7xhjzxhjz3HH7Jp6HABYibbhBoCbIuEGgEaEGwAaEW4AaGTdL8DyxKlnAIB1YsUNAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajaxnuqjq3qs6Yeg4AWDdrGW4AYHsHDHdVPaKqvlZVu+ef36mqRlX91pbHvLCq3l1Vu6rq9VX1mar6RlV9sqqeW1VHbHnsmVX1jqp6ZlVdVFWXV9Ubq+ro/fcneXCSZ8z/O6OqTr6Rv28AaGn3QTzmvUmOSrInyQeSPCTJJUkeuuUxD0nyx5n9InBRkscm2Zfkvkn2Jrk0yeu3PP6BSb6U5NQkJyV5S5JPJHlRkmcmuXOSv0ry7+aP33eI3xcAbKQDrrjHGFck+UiuC/VDkpyR5A5Vddv5Svk+Sc4dY1w9xnj+GONDY4zPjjHekuR1SX5i4a/9apKnjzH+7xjjT5P8lyQPm//3vpLkm0muHGN8eX67dnGuqjq9qs6vqvP3XfoP7gaAjXSwz3Gfm1mwk9lh7P+R5Lz5tgckuXr+earqafOg7quqK5I8O8ntF/6+C8YY12z5/ItJbnMog48x9o4x9owx9hx3zK5D+VIAaOtQwv2Aqrprklsl+fB820Mzi/dfjDGurqofT/LKJGcmOS3JvZK8NsnNFv6+qxc+H4cwCwDcZB3Mc9zJ7Hnumyd5bpI/H2NcW1XnZvb89cWZPb+dJD+U5INjjG+/lKuqTtnBXN9MYhkNAAsOapW75Xnuxyc5Z775/ZmdWHa/zFbfyewEs3vPz0S/U1U9L7ND64fqs0nuW1UnV9WxW89KB4CbskMJ4jmZrYLPTZIxxt9ldpb5VZk/v53ktzM7Q/zNST6U5OQkL9/BXC/LbNV9QWZnlC8+Rw4AN0k1xph6hsO2555HjfPeddLUYwBsvNNOuNfUI2ys94y3fniMsedAj3MIGgAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoZK3CXVUPr6r3VtXlVXVZVb2rqu4y9VwAsC7WKtxJviPJK5PcN8lDknwlydur6mZTDgUA62L31ANsNcb4w62fV9VPJflqZiH/84X7Tk9yepLc/sS1+jYAYGnWasVdVadU1Zur6lNV9dUkf5vZjLdffOwYY+8YY88YY89xx+xa+awAMIV1W6q+PclFSZ46//OaJBckcagcALJG4a6qY5LcJckzxhjnzLfdO2s0IwBMbZ2ieHmSS5I8paouTHJikpdmtuoGALJGz3GPMb6V5MeT3CPJx5O8Jsnzklw15VwAsE7WacWdMcbZSf7JwuZbTjELAKyjtVlxAwAHJtwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0AjhxTuqjq3qs5Y1jAAwA2z4gaARtY+3FV1s6lnAIB1sZNw766qV1XV5fPbS6vqiGQW2ap6cVV9oaq+XlUfqqrTtn5xVd21qt5ZVV+rqour6veq6vgt959ZVe+oql+oqi8k+cLhfYsAsDl2Eu7Hzb/u/kmemuT0JM+a3/fGJA9O8pNJ7p7kTUneXlX3TJKqum2SP0vy8ST3TXJqklsmedv++M89OMk9kjw8ycN2MCMAbKTdO/iaLyX52THGSPJXVXXnJM+pqv+e5CeSnDzG+Pz8sWdU1amZBf6nkzw9yV+OMX5h/19WVf8yyWVJ9iQ5b77575I8aYxx1fUNUVWnZ/ZLQ25/4k6+DQDoZycr7g/Mo73f+5OcmOSHklSSC6rqiv23JD+S5JT5Y78/yYMW7r9wft8pW/7Oj99QtJNkjLF3jLFnjLHnuGN27eDbAIB+buyl6khynyRXL2z/xvzPI5K8M8nPb/O1f7vl46/fyHMBwEbYSbjvV1W1ZdX9A0m+mNnKu5IcP8Y453q+9iNJHpvkc2OMxbgDAAewk0PlJyR5ZVV9X1U9Jsm/TfKKMcYnkpyV5MyqekxV3bGq9lTVz1fVo+df+5ok35nkD6rqfvPHnFpVe6vqVjfKdwQAG2wnK+6zkuxK8sHMDo2/Pskr5vf9VJJfTvKSJLfL7KSz85KckyRjjC9W1QOSvCjJnyQ5Ksnnk/xpkht8ThsAOMRwjzEesuXTn9nm/quTvGB+u76/45NJHnMD9z/xUGYCgJuStb9yGgBwHeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaGRtwl1VZ1bV2Ob2galnA4B1sXvqARa8J8kTFrZ9c4pBAGAdrVu4rxpjfHnqIQBgXa3NoXIA4MDWLdwPr6orFm4v3u6BVXV6VZ1fVefvu/TaVc8JAJNYt0Plf5bk9IVt/2+7B44x9ibZmyR77nnUWPJcALAW1i3cV44x/mbqIQBgXa3boXIA4Aas24r75lV1/MK2a8cY+yaZBgDWzLqF+9QkX1rYdlGS200wCwCsnbU5VD7GeOIYo7a5iTYAzK1NuAGAAxNuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGaowx9QyHrar2Jfnc1HMcpGOTXDL1EBvKvl0e+3Z57Nvl6bZv7zDGOO5AD9qIcHdSVeePMfZMPccmsm+Xx75dHvt2eTZ13zpUDgCNCDcANCLcq7d36gE2mH27PPbt8ti3y7OR+9Zz3ADQiBU3ADQi3ADQiHADQCPCDQCNCDcANPL/AYflpOuMtLnuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_enc_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "# Transformer Parameters\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'S' : 5, 'E' : 6}\n",
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5\n",
    "tgt_len = 5\n",
    "\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048 # FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "  \n",
    "  \n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "      \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "      \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "  \n",
    "####################### COMPLETED UPTO THIS #####################  \n",
    "  \n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return nn.LayerNorm(d_model)(output + residual)\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()# into the attention network\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()# then into the feed forward network\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_vocab_size, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_vocab_size, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "    loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "# Test\n",
    "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "print('first head of last state enc_self_attns')\n",
    "showgraph(enc_self_attns)\n",
    "\n",
    "print('first head of last state dec_self_attns')\n",
    "showgraph(dec_self_attns)\n",
    "\n",
    "print('first head of last state dec_enc_attns')\n",
    "showgraph(dec_enc_attns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2_oOUgi7sM7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
